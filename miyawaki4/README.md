
```sh
# Load model
decoder = MiyawakiDecoder()
decoder.load_model("miyawaki_contrastive_clip.pth")

# Use for inference
fmri_input = your_fmri_data  # Shape: (1, 967)
clip_embedding = decoder.fmri_encoder(fmri_input)  # Shape: (1, 512)

# Compare dengan CLIP image embeddings
image_embeddings = clip_model.encode_image(images)  # Shape: (N, 512)
similarities = torch.matmul(clip_embedding, image_embeddings.T)  # Shape: (1, N)
```


ğŸ” CLIP MODEL TIDAK TERSIMPAN DALAM .PTH
âŒ YANG TIDAK ADA DALAM .PTH:
ğŸš« CLIP Model Components:

âœ… CLIP Vision Encoder - TIDAK tersimpan
âœ… CLIP Text Encoder - TIDAK tersimpan
âœ… CLIP Preprocessing - TIDAK tersimpan
âœ… YANG ADA DALAM .PTH:
Mari saya tunjukkan dengan kode:

Read lines 295-301
runembedding.py
miyawaki4
Sekarang mari lihat bagaimana CLIP diinisialisasi:

Read lines 122-139
runembedding.py
miyawaki4
ğŸ“¦ JAWABAN LENGKAP: CLIP TIDAK TERSIMPAN DALAM .PTH
âŒ CLIP MODEL TIDAK ADA DALAM .PTH:
ğŸ“ Contents of miyawaki_contrastive_clip.pth:

{
    'fmri_encoder_state_dict': {...},  # âœ… HANYA fMRI encoder weights
    'scaler': StandardScaler()         # âœ… HANYA data normalization
}
ğŸš« YANG TIDAK TERSIMPAN:

âŒ CLIP Vision Encoder (ViT-B/32)
âŒ CLIP Text Encoder
âŒ CLIP Preprocessing parameters
âŒ CLIP model weights
ğŸ”„ MENGAPA CLIP TIDAK DISIMPAN:
1. ğŸ”’ CLIP Model di-Freeze:

# Line 127-129
for param in self.clip_model.parameters():
    param.requires_grad = False  # CLIP weights tidak berubah
2. ğŸ“¥ CLIP Dimuat Ulang Setiap Kali:

# Line 125 - setiap kali load model
self.clip_model, self.clip_preprocess = clip.load("ViT-B/32", device=self.device)
3. ğŸ¯ Hanya fMRI Encoder yang Ditraining:

# Line 163 - hanya fMRI encoder yang dioptimize
optimizer = optim.Adam(self.fmri_encoder.parameters(), lr=lr, weight_decay=1e-4)
ğŸ”„ PROSES LOAD MODEL:
ğŸ“‹ Ketika Load Model:

def load_model(self, filepath):
    checkpoint = torch.load(filepath)
    
    if self.fmri_encoder is None:
        self.initialize_models()  # â† CLIP dimuat ulang di sini!
    
    self.fmri_encoder.load_state_dict(checkpoint['fmri_encoder_state_dict'])
    self.scaler = checkpoint['scaler']
ğŸ”„ initialize_models() dipanggil yang akan:

# Download/load CLIP model dari internet/cache
self.clip_model, self.clip_preprocess = clip.load("ViT-B/32", device=self.device)
ğŸ“Š SIZE COMPARISON:
ğŸ“¦ File Sizes:

ğŸ’¡ IMPLICATIONS:
âœ… ADVANTAGES:

Smaller file size (18.5 MB vs 350+ MB)
Always latest CLIP (auto-download updates)
Consistent CLIP version across environments
Only custom weights saved (fMRI encoder)
âš ï¸ REQUIREMENTS:

Internet connection untuk first-time CLIP download
CLIP library harus terinstall
CLIP cache akan tersimpan di system (~350 MB)
ğŸ¯ SUMMARY:
ğŸ”‘ KEY POINTS:

âœ… .pth hanya berisi fMRI encoder + scaler
âŒ CLIP model TIDAK tersimpan dalam .pth
ğŸ”„ CLIP dimuat ulang setiap kali load model
ğŸ”’ CLIP di-freeze, tidak berubah selama training
ğŸ“¦ File size kecil karena hanya custom weights
ğŸ§  ANALOGI:
Seperti menyimpan "adapter" yang menghubungkan brain ke CLIP, tapi CLIP-nya sendiri tetap diambil dari "perpustakaan" (OpenAI) setiap kali dibutuhkan.

Model .pth ini adalah "brain-to-CLIP translator" bukan "complete brain-image system"! ğŸ§ â†’ğŸ”—â†’ğŸ–¼ï¸