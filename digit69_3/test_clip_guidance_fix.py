#!/usr/bin/env python3
"""
Test CLIP Guidance Fix
Verify that the gradient computation error is resolved
"""

import torch
import torch.nn.functional as F
from improved_clip_v2 import create_improved_clip_v2_model

def test_clip_guidance_fix():
    """Test the fixed CLIP guidance implementation"""
    print("ğŸ”§ TESTING CLIP GUIDANCE FIX")
    print("=" * 50)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ“± Device: {device}")
    
    # Create model
    print("ğŸ—ï¸ Creating Improved CLIP v2.0 model...")
    model = create_improved_clip_v2_model().to(device)
    
    # Load trained weights
    print("ğŸ“‚ Loading trained weights...")
    try:
        checkpoint = torch.load('improved_clip_v2_best.pth', map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
        print("âœ… Model loaded successfully")
    except Exception as e:
        print(f"âŒ Error loading model: {e}")
        return False
    
    # Test data
    print("\nğŸ§ª Preparing test data...")
    batch_size = 1
    fmri_condition = torch.randn(batch_size, 512).to(device)
    text_prompts = ["a handwritten digit 3"]
    
    print(f"   fMRI condition shape: {fmri_condition.shape}")
    print(f"   Text prompts: {text_prompts}")
    
    # Test 1: Pure diffusion sampling (should work)
    print("\nğŸ¯ Test 1: Pure Diffusion Sampling...")
    try:
        with torch.no_grad():
            pure_sample = model.sample(
                fmri_condition, 
                text_prompts=None,
                num_samples=1, 
                num_timesteps=10  # Very fast test
            )
        print(f"âœ… Pure diffusion successful! Shape: {pure_sample.shape}")
        print(f"   Sample range: [{pure_sample.min():.3f}, {pure_sample.max():.3f}]")
    except Exception as e:
        print(f"âŒ Pure diffusion failed: {e}")
        return False
    
    # Test 2: CLIP guidance sampling (the critical test)
    print("\nğŸ¯ Test 2: CLIP Guidance Sampling...")
    try:
        with torch.no_grad():
            clip_sample = model.sample(
                fmri_condition, 
                text_prompts=text_prompts,
                num_samples=1, 
                num_timesteps=10,  # Very fast test
                clip_guidance_scale=0.01,
                clip_start_step=5
            )
        print(f"âœ… CLIP guidance successful! Shape: {clip_sample.shape}")
        print(f"   Sample range: [{clip_sample.min():.3f}, {clip_sample.max():.3f}]")
    except Exception as e:
        print(f"âŒ CLIP guidance failed: {e}")
        print(f"   Error details: {str(e)}")
        import traceback
        traceback.print_exc()
        return False
    
    # Test 3: Direct CLIP guidance computation
    print("\nğŸ¯ Test 3: Direct CLIP Guidance Computation...")
    try:
        # Create a test image
        test_image = torch.randn(1, 1, 28, 28).to(device)
        test_image.requires_grad_(True)
        
        # Compute CLIP guidance
        clip_grad = model.compute_clip_guidance(
            test_image, 
            text_prompts, 
            guidance_scale=0.01
        )
        
        print(f"âœ… CLIP guidance computation successful!")
        print(f"   Gradient shape: {clip_grad.shape}")
        print(f"   Gradient range: [{clip_grad.min():.6f}, {clip_grad.max():.6f}]")
        print(f"   Gradient norm: {clip_grad.norm():.6f}")
        
    except Exception as e:
        print(f"âŒ CLIP guidance computation failed: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # Test 4: Compare pure vs CLIP guided samples
    print("\nğŸ¯ Test 4: Comparing Pure vs CLIP Guided Samples...")
    try:
        # Generate longer samples for comparison
        with torch.no_grad():
            pure_sample_long = model.sample(
                fmri_condition, 
                text_prompts=None,
                num_samples=1, 
                num_timesteps=50
            )
            
            clip_sample_long = model.sample(
                fmri_condition, 
                text_prompts=text_prompts,
                num_samples=1, 
                num_timesteps=50,
                clip_guidance_scale=0.01,
                clip_start_step=25
            )
        
        # Compute difference
        diff = torch.abs(pure_sample_long - clip_sample_long).mean()
        print(f"âœ… Comparison successful!")
        print(f"   Pure sample range: [{pure_sample_long.min():.3f}, {pure_sample_long.max():.3f}]")
        print(f"   CLIP sample range: [{clip_sample_long.min():.3f}, {clip_sample_long.max():.3f}]")
        print(f"   Mean absolute difference: {diff:.6f}")
        
        if diff > 0.001:
            print(f"âœ… CLIP guidance is having an effect (difference > 0.001)")
        else:
            print(f"âš ï¸  CLIP guidance effect is minimal (difference â‰¤ 0.001)")
            
    except Exception as e:
        print(f"âŒ Comparison failed: {e}")
        return False
    
    print("\nğŸ‰ ALL TESTS PASSED!")
    print("âœ… CLIP guidance fix is working correctly")
    return True

def test_clip_model_states():
    """Test CLIP model state changes during guidance"""
    print("\nğŸ” TESTING CLIP MODEL STATE MANAGEMENT")
    print("=" * 50)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = create_improved_clip_v2_model().to(device)
    
    # Check initial state
    print("ğŸ“Š Initial CLIP model state:")
    print(f"   Training mode: {model.clip_model.training}")
    print(f"   Parameters require grad: {any(p.requires_grad for p in model.clip_model.parameters())}")
    
    # Test guidance computation
    test_image = torch.randn(1, 1, 28, 28).to(device)
    text_prompts = ["a handwritten digit 5"]
    
    print("\nğŸ”§ During CLIP guidance computation...")
    try:
        clip_grad = model.compute_clip_guidance(test_image, text_prompts)
        print("âœ… Guidance computation successful")
    except Exception as e:
        print(f"âŒ Guidance computation failed: {e}")
        return False
    
    # Check final state
    print("\nğŸ“Š Final CLIP model state:")
    print(f"   Training mode: {model.clip_model.training}")
    print(f"   Parameters require grad: {any(p.requires_grad for p in model.clip_model.parameters())}")
    
    print("âœ… CLIP model state management working correctly")
    return True

if __name__ == "__main__":
    print("ğŸš€ STARTING CLIP GUIDANCE FIX VERIFICATION")
    print("=" * 60)
    
    # Test the fix
    success = test_clip_guidance_fix()
    
    if success:
        # Test state management
        test_clip_model_states()
        
        print("\nğŸ¯ SUMMARY:")
        print("âœ… CLIP guidance gradient computation error FIXED!")
        print("âœ… Model can now perform CLIP-guided sampling")
        print("âœ… Ready for comprehensive evaluation")
        
        print("\nğŸš€ NEXT STEPS:")
        print("   1. Run full evaluation with CLIP guidance")
        print("   2. Compare Pure vs CLIP guided performance")
        print("   3. Test different guidance scales")
        print("   4. Evaluate semantic improvements")
        
    else:
        print("\nâŒ CLIP GUIDANCE FIX VERIFICATION FAILED")
        print("   Additional debugging required")
