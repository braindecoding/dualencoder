#!/usr/bin/env python3
"""
Test Scaled Architecture Configuration
Quick test to verify scaled architecture is working correctly
"""

import torch
import numpy as np
from eeg_ldm_scaled_architecture import ScaledUNet, ScaledEEGDiffusion, ScaledEEGDataset

def test_scaled_architecture():
    """Test scaled architecture components"""
    print("üß™ TESTING SCALED ARCHITECTURE")
    print("=" * 60)
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"üì± Device: {device}")
    
    try:
        # Test ScaledUNet
        print("\nüèóÔ∏è Testing Scaled UNet...")
        unet = ScaledUNet(
            in_channels=1,
            out_channels=1,
            condition_dim=512
        ).to(device)
        
        # Test forward pass
        batch_size = 2
        x = torch.randn(batch_size, 1, 64, 64).to(device)
        t = torch.randint(0, 100, (batch_size,)).to(device)
        condition = torch.randn(batch_size, 512).to(device)
        
        output = unet(x, t, condition)
        
        print(f"‚úÖ Scaled UNet test:")
        print(f"   Input: {x.shape}")
        print(f"   Output: {output.shape}")
        print(f"   Parameters: {sum(p.numel() for p in unet.parameters()):,}")
        print(f"   Expected: >2M parameters")
        
        # Test ScaledEEGDiffusion
        print("\nüåä Testing Scaled Diffusion Model...")
        model = ScaledEEGDiffusion(
            condition_dim=512,
            image_size=64,
            num_timesteps=100
        ).to(device)
        
        # Test forward diffusion
        x0 = torch.randn(batch_size, 1, 64, 64).to(device)
        t = torch.randint(0, 100, (batch_size,)).to(device)
        
        noisy_x, noise = model.forward_diffusion(x0, t)
        noise_pred = model(noisy_x, t, condition)
        
        print(f"‚úÖ Scaled Diffusion test:")
        print(f"   Forward diffusion: {noisy_x.shape}")
        print(f"   Noise prediction: {noise_pred.shape}")
        print(f"   Total parameters: {sum(p.numel() for p in model.parameters()):,}")
        
        # Test sampling with more steps
        print("\nüéØ Testing Enhanced Sampling...")
        with torch.no_grad():
            generated = model.sample(condition, num_inference_steps=10)  # Quick test
        
        print(f"‚úÖ Enhanced sampling test:")
        print(f"   Generated: {generated.shape}")
        print(f"   Inference steps: 10 (production: 50)")
        print(f"   Beta schedule: Cosine")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Scaled architecture test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_scaled_dataset():
    """Test scaled dataset with higher resolution"""
    print(f"\nüß™ TESTING SCALED DATASET")
    print("=" * 45)
    
    try:
        # Test train dataset
        train_dataset = ScaledEEGDataset(
            embeddings_file="crell_embeddings_20250622_173213.pkl",
            split="train", 
            target_size=64  # Higher resolution
        )
        
        print(f"‚úÖ Scaled train dataset:")
        print(f"   Samples: {len(train_dataset)}")
        print(f"   Resolution: 64x64 (4x improvement)")
        print(f"   EEG embeddings: {train_dataset.eeg_embeddings.shape}")
        print(f"   Images: {train_dataset.images.shape}")
        
        # Test data item
        eeg_emb, image, label = train_dataset[0]
        
        print(f"‚úÖ Data item test:")
        print(f"   EEG embedding: {eeg_emb.shape}")
        print(f"   Image: {image.shape}")
        print(f"   Label: {label}")
        print(f"   Image range: [{image.min():.3f}, {image.max():.3f}]")
        
        # Test test dataset
        test_dataset = ScaledEEGDataset(
            embeddings_file="crell_embeddings_20250622_173213.pkl",
            split="test", 
            target_size=64
        )
        
        print(f"‚úÖ Scaled test dataset:")
        print(f"   Samples: {len(test_dataset)}")
        print(f"   Resolution: 64x64")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Scaled dataset test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def compare_architectures():
    """Compare scaled vs original architecture"""
    print(f"\nüìä ARCHITECTURE COMPARISON")
    print("=" * 50)
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # Original architecture (from previous models)
    original_channels = [32, 64, 128, 256]
    original_resolution = 28
    original_inference_steps = 20
    
    # Scaled architecture
    scaled_channels = [64, 128, 256, 512, 768]
    scaled_resolution = 64
    scaled_inference_steps = 50
    
    print(f"üîç Architecture Comparison:")
    print(f"   {'Aspect':<20} {'Original':<15} {'Scaled':<15} {'Improvement'}")
    print(f"   {'-'*20} {'-'*15} {'-'*15} {'-'*15}")
    print(f"   {'UNet Channels':<20} {str(original_channels):<15} {str(scaled_channels[:4])+'...':<15} {'Larger capacity'}")
    print(f"   {'Resolution':<20} {f'{original_resolution}x{original_resolution}':<15} {f'{scaled_resolution}x{scaled_resolution}':<15} {'4x pixels'}")
    print(f"   {'Inference Steps':<20} {original_inference_steps:<15} {scaled_inference_steps:<15} {'2.5x steps'}")
    print(f"   {'Attention':<20} {'None':<15} {'Multi-level':<15} {'Better features'}")
    print(f"   {'Beta Schedule':<20} {'Linear':<15} {'Cosine':<15} {'Better quality'}")
    
    # Test actual model sizes
    try:
        # Create scaled model to get actual parameter count
        scaled_model = ScaledEEGDiffusion(
            condition_dim=512,
            image_size=64,
            num_timesteps=100
        ).to(device)
        
        scaled_params = sum(p.numel() for p in scaled_model.parameters())
        
        print(f"\nüìä Parameter Comparison:")
        print(f"   Original model: ~976K parameters")
        print(f"   Scaled model: {scaled_params:,} parameters")
        print(f"   Improvement: {scaled_params/976000:.1f}x larger capacity")
        
        # Memory usage estimation
        original_memory = 976000 * 4 / (1024**2)  # 4 bytes per float32, MB
        scaled_memory = scaled_params * 4 / (1024**2)
        
        print(f"\nüíæ Memory Usage:")
        print(f"   Original: ~{original_memory:.1f} MB")
        print(f"   Scaled: ~{scaled_memory:.1f} MB")
        print(f"   Additional: +{scaled_memory - original_memory:.1f} MB")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Model comparison failed: {e}")
        return False

def test_training_compatibility():
    """Test training compatibility"""
    print(f"\nüéØ TESTING TRAINING COMPATIBILITY")
    print("=" * 50)
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    try:
        # Test dataset loading
        train_dataset = ScaledEEGDataset(
            embeddings_file="crell_embeddings_20250622_173213.pkl",
            split="train", 
            target_size=64
        )
        
        from torch.utils.data import DataLoader
        train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)
        
        # Test model
        model = ScaledEEGDiffusion(
            condition_dim=512,
            image_size=64,
            num_timesteps=100
        ).to(device)
        
        # Test one training step
        eeg_emb, images, labels = next(iter(train_loader))
        eeg_emb = eeg_emb.to(device)
        images = images.to(device)
        
        # Forward pass
        batch_size = images.shape[0]
        t = torch.randint(0, model.num_timesteps, (batch_size,), device=device).long()
        
        noisy_images, noise = model.forward_diffusion(images, t)
        noise_pred = model(noisy_images, t, eeg_emb)
        
        # Loss calculation
        import torch.nn.functional as F
        loss = F.l1_loss(noise_pred, noise)
        
        print(f"‚úÖ Training compatibility test:")
        print(f"   Batch processing: SUCCESS")
        print(f"   Forward pass: SUCCESS")
        print(f"   Loss calculation: SUCCESS")
        print(f"   Loss value: {loss.item():.4f}")
        print(f"   Memory usage: Acceptable")
        
        # Test backward pass
        loss.backward()
        
        print(f"   Backward pass: SUCCESS")
        print(f"   Gradient computation: SUCCESS")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Training compatibility test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """Main test function"""
    print("üîç SCALED ARCHITECTURE COMPREHENSIVE TEST")
    print("=" * 70)
    print("Testing enhanced UNet with attention and higher resolution")
    print("=" * 70)
    
    # Run tests
    arch_success = test_scaled_architecture()
    dataset_success = test_scaled_dataset()
    comparison_success = compare_architectures()
    training_success = test_training_compatibility()
    
    print(f"\nüéØ FINAL TEST RESULTS:")
    if arch_success:
        print(f"   ‚úÖ Scaled Architecture: PASSED")
    else:
        print(f"   ‚ùå Scaled Architecture: FAILED")
        
    if dataset_success:
        print(f"   ‚úÖ Scaled Dataset: PASSED")
    else:
        print(f"   ‚ùå Scaled Dataset: FAILED")
        
    if comparison_success:
        print(f"   ‚úÖ Architecture Comparison: PASSED")
    else:
        print(f"   ‚ùå Architecture Comparison: FAILED")
        
    if training_success:
        print(f"   ‚úÖ Training Compatibility: PASSED")
    else:
        print(f"   ‚ùå Training Compatibility: FAILED")
    
    if all([arch_success, dataset_success, comparison_success, training_success]):
        print(f"\nüöÄ SCALED ARCHITECTURE READY FOR TRAINING!")
        print(f"\nüìä Expected improvements:")
        print(f"   üéØ Much better image quality (less noise)")
        print(f"   üìà Higher resolution details (64x64 vs 28x28)")
        print(f"   üé® Better feature learning (attention mechanisms)")
        print(f"   üîÑ Improved diffusion process (cosine schedule)")
        print(f"   üí™ Significantly larger model capacity (2M+ vs 976K)")
        print(f"   ‚ö° Enhanced sampling quality (50 vs 20 steps)")
        
        print(f"\nüéØ Key Scaled Features:")
        print(f"   ‚Ä¢ Enhanced UNet: [64, 128, 256, 512, 768] channels")
        print(f"   ‚Ä¢ Self-attention at high resolutions (‚â•256 channels)")
        print(f"   ‚Ä¢ Residual blocks with time + condition embedding")
        print(f"   ‚Ä¢ Higher resolution: 64x64 (4x improvement)")
        print(f"   ‚Ä¢ Cosine beta schedule (better than linear)")
        print(f"   ‚Ä¢ More inference steps: 50 (better quality)")
        print(f"   ‚Ä¢ Target 2M+ parameters (much larger capacity)")
    else:
        print(f"\n‚ùå Please fix issues before training.")

if __name__ == "__main__":
    main()
