#!/usr/bin/env python3
"""
CLIP-Guided EEG LDM for Semantic Accuracy
Adds CLIP guidance to ensure generated images match semantic content
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
import pickle
import time
import math
from sklearn.metrics import mean_squared_error
from scipy.stats import pearsonr

# Import CLIP
try:
    import clip
    CLIP_AVAILABLE = True
    print("âœ… CLIP library available")
except ImportError:
    CLIP_AVAILABLE = False
    print("âš ï¸ CLIP not available, using alternative approach")

# Import improved LDM components
from eeg_ldm_improved import (
    SinusoidalPositionEmbeddings,
    SimpleResBlock,
    ImprovedUNet
)

class CLIPGuidedEEGDataset(Dataset):
    """Dataset for CLIP-guided EEG LDM training with Crell embeddings"""

    def __init__(self, embeddings_file="crell_embeddings_20250622_173213.pkl",
                 split="train", target_size=28):
        self.split = split
        self.target_size = target_size

        # Load Crell embeddings and data
        with open(embeddings_file, 'rb') as f:
            emb_data = pickle.load(f)

        # Extract embeddings and labels directly
        all_embeddings = emb_data['embeddings']
        all_labels = emb_data['labels']

        # Load original Crell data for stimulus images
        crell_data_file = "crell_processed_data_correct.pkl"
        with open(crell_data_file, 'rb') as f:
            crell_data = pickle.load(f)

        # Get stimulus images from validation set (same as embeddings)
        all_images = crell_data['validation']['images']

        # Split data for train/val since Crell only has validation data
        n_samples = len(all_embeddings)
        if split == "train":
            # Use first 80% for training
            end_idx = int(0.8 * n_samples)
            self.eeg_embeddings = all_embeddings[:end_idx]
            self.labels = all_labels[:end_idx]
            self.original_images = all_images[:end_idx]
        else:  # val/test
            # Use last 20% for validation/testing
            start_idx = int(0.8 * n_samples)
            self.eeg_embeddings = all_embeddings[start_idx:]
            self.labels = all_labels[start_idx:]
            self.original_images = all_images[start_idx:]

        print(f"ðŸ“Š Loaded {split} data:")
        print(f"   EEG embeddings: {self.eeg_embeddings.shape}")
        print(f"   Labels: {len(self.labels)}")
        print(f"   Original images: {len(self.original_images)} images")

        # Process images
        self.images = self._process_images()

        print(f"   Processed images: {self.images.shape}")
        print(f"   Image range: [{self.images.min():.3f}, {self.images.max():.3f}]")

    def _process_images(self):
        """Process images to target format"""
        import numpy as np
        from PIL import Image

        processed_images = []

        for img in self.original_images:
            # Handle different image types
            if hasattr(img, 'mode'):  # PIL Image
                img_array = np.array(img)
            else:  # numpy array
                img_array = img

            # Convert to grayscale if RGB
            if len(img_array.shape) == 3:
                if img_array.shape[2] == 3:  # RGB
                    img_array = np.mean(img_array, axis=2)
                elif img_array.shape[2] == 1:  # Single channel
                    img_array = img_array[:, :, 0]

            # Ensure 2D
            if len(img_array.shape) > 2:
                img_array = img_array.squeeze()

            # Resize to target size if needed
            if img_array.shape[0] != self.target_size or img_array.shape[1] != self.target_size:
                pil_img = Image.fromarray(img_array.astype(np.uint8))
                pil_resized = pil_img.resize((self.target_size, self.target_size), Image.LANCZOS)
                img_array = np.array(pil_resized)

            # Normalize to [-1, 1]
            img_array = img_array.astype(np.float32)
            if img_array.max() > 1.0:  # If not already normalized
                img_array = img_array / 255.0  # [0, 1]
            img_array = (img_array - 0.5) * 2  # [-1, 1]

            processed_images.append(img_array)

        # Stack and add channel dimension
        images = np.array(processed_images)  # (N, H, W)
        images = images[:, None, :, :]  # (N, 1, H, W)

        return images

    def __len__(self):
        return len(self.eeg_embeddings)

    def __getitem__(self, idx):
        eeg_emb = torch.FloatTensor(self.eeg_embeddings[idx])
        image = torch.FloatTensor(self.images[idx])
        label = self.labels[idx]

        return eeg_emb, image, label

class CLIPGuidedLoss(nn.Module):
    """CLIP-guided loss for semantic accuracy"""
    
    def __init__(self, device='cuda'):
        super().__init__()
        self.device = device
        
        if CLIP_AVAILABLE:
            # Load CLIP model
            self.clip_model, self.clip_preprocess = clip.load("ViT-B/32", device=device)
            self.clip_model.eval()
            
            # Letter text prompts for Crell dataset
            # Mapping: 0â†’'a', 1â†’'d', 2â†’'e', 3â†’'f', 4â†’'j', 5â†’'n', 6â†’'o', 7â†’'s', 8â†’'t', 9â†’'v'
            self.letter_mapping = {0: 'a', 1: 'd', 2: 'e', 3: 'f', 4: 'j', 5: 'n', 6: 'o', 7: 's', 8: 't', 9: 'v'}
            self.letter_texts = [
                f"a handwritten letter {self.letter_mapping[i]}" for i in range(10)
            ]
            
            # Encode text prompts
            with torch.no_grad():
                text_tokens = clip.tokenize(self.letter_texts).to(device)
                self.text_features = self.clip_model.encode_text(text_tokens)
                self.text_features = F.normalize(self.text_features, dim=-1)
            
            print(f"ðŸ“Š CLIP text features: {self.text_features.shape}")
        else:
            # Fallback: Simple classification loss
            self.classifier = nn.Sequential(
                nn.Conv2d(1, 32, 3, padding=1),
                nn.ReLU(),
                nn.MaxPool2d(2),
                nn.Conv2d(32, 64, 3, padding=1),
                nn.ReLU(),
                nn.MaxPool2d(2),
                nn.Flatten(),
                nn.Linear(64 * 7 * 7, 128),
                nn.ReLU(),
                nn.Linear(128, 10)  # 10 letters: a,d,e,f,j,n,o,s,t,v
            ).to(device)
            
            print("ðŸ“Š Using classification loss as CLIP alternative")
    
    def forward(self, generated_images, target_labels):
        """Calculate CLIP-guided loss"""
        if CLIP_AVAILABLE:
            return self._clip_loss(generated_images, target_labels)
        else:
            return self._classification_loss(generated_images, target_labels)
    
    def _clip_loss(self, generated_images, target_labels):
        """CLIP-based semantic loss"""
        
        # Resize images to 224x224 for CLIP
        resized_images = F.interpolate(
            generated_images, 
            size=(224, 224), 
            mode='bilinear', 
            align_corners=False
        )
        
        # Convert grayscale to RGB
        rgb_images = resized_images.repeat(1, 3, 1, 1)
        
        # Normalize for CLIP (assuming images are in [-1, 1])
        rgb_images = (rgb_images + 1) / 2  # Convert to [0, 1]
        
        # Encode images
        with torch.no_grad():
            image_features = self.clip_model.encode_image(rgb_images)
            image_features = F.normalize(image_features, dim=-1)
        
        # Get target text features
        target_text_features = self.text_features[target_labels]
        
        # Calculate cosine similarity
        similarities = torch.sum(image_features * target_text_features, dim=-1)
        
        # Convert to loss (maximize similarity = minimize negative similarity)
        clip_loss = -similarities.mean()
        
        return clip_loss
    
    def _classification_loss(self, generated_images, target_labels):
        """Classification-based semantic loss"""
        logits = self.classifier(generated_images)
        return F.cross_entropy(logits, target_labels)

class CLIPGuidedEEGDiffusion(nn.Module):
    """EEG diffusion model with CLIP guidance"""
    
    def __init__(self, condition_dim=512, image_size=28, num_timesteps=100):
        super().__init__()
        
        self.condition_dim = condition_dim
        self.image_size = image_size
        self.num_timesteps = num_timesteps
        
        # UNet for noise prediction
        self.unet = ImprovedUNet(
            in_channels=1,
            out_channels=1,
            condition_dim=condition_dim
        )
        
        # Diffusion schedule
        self.register_buffer('betas', self._linear_beta_schedule(num_timesteps))
        self.register_buffer('alphas', 1.0 - self.betas)
        self.register_buffer('alphas_cumprod', torch.cumprod(self.alphas, dim=0))
        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(self.alphas_cumprod))
        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1.0 - self.alphas_cumprod))
        
        print(f"ðŸ“Š CLIP-Guided EEG Diffusion Model:")
        print(f"   Total parameters: {sum(p.numel() for p in self.parameters()):,}")
        
    def _linear_beta_schedule(self, timesteps):
        """Linear beta schedule"""
        beta_start = 0.0001
        beta_end = 0.02
        return torch.linspace(beta_start, beta_end, timesteps)
    
    def forward_diffusion(self, x0, t, noise=None):
        """Add noise to images"""
        if noise is None:
            noise = torch.randn_like(x0)
        
        sqrt_alphas_cumprod_t = self.sqrt_alphas_cumprod[t][:, None, None, None]
        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t][:, None, None, None]
        
        return sqrt_alphas_cumprod_t * x0 + sqrt_one_minus_alphas_cumprod_t * noise, noise
    
    def forward(self, x, t, condition):
        """Predict noise"""
        return self.unet(x, t, condition)
    
    @torch.no_grad()
    def sample(self, condition, shape=None, num_inference_steps=20):
        """Generate image from EEG condition"""
        if shape is None:
            shape = (condition.shape[0], 1, self.image_size, self.image_size)
        
        device = condition.device
        
        # Start from noise
        x = torch.randn(shape, device=device)
        
        # DDIM sampling
        timesteps = torch.linspace(self.num_timesteps - 1, 0, num_inference_steps, dtype=torch.long, device=device)
        
        for i, t in enumerate(tqdm(timesteps, desc="Sampling", leave=False)):
            t_batch = t.repeat(shape[0])
            
            # Predict noise
            noise_pred = self.unet(x, t_batch, condition)
            
            # DDIM step
            if i < len(timesteps) - 1:
                alpha_t = self.alphas_cumprod[t]
                alpha_t_next = self.alphas_cumprod[timesteps[i + 1]]
                
                x = torch.sqrt(alpha_t_next) * (x - torch.sqrt(1 - alpha_t) * noise_pred) / torch.sqrt(alpha_t) + \
                    torch.sqrt(1 - alpha_t_next) * noise_pred
            else:
                # Final step
                alpha_t = self.alphas_cumprod[t]
                x = (x - torch.sqrt(1 - alpha_t) * noise_pred) / torch.sqrt(alpha_t)
        
        return torch.clamp(x, -1, 1)

def train_clip_guided_ldm():
    """Train CLIP-guided EEG LDM"""
    print("ðŸš€ TRAINING CLIP-GUIDED EEG LDM")
    print("=" * 60)
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ðŸ“± Device: {device}")
    
    # Load datasets with Crell embeddings
    train_dataset = CLIPGuidedEEGDataset(
        embeddings_file="crell_embeddings_20250622_173213.pkl",
        split="train",
        target_size=28
    )
    test_dataset = CLIPGuidedEEGDataset(
        embeddings_file="crell_embeddings_20250622_173213.pkl",
        split="test",
        target_size=28
    )
    
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)
    
    # Initialize models
    model = CLIPGuidedEEGDiffusion(
        condition_dim=512, 
        image_size=28, 
        num_timesteps=100
    ).to(device)
    
    clip_loss_fn = CLIPGuidedLoss(device=device)
    
    # Training setup
    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)
    
    # Training parameters
    num_epochs = 50
    best_test_loss = float('inf')
    train_losses = []
    test_losses = []
    
    # Loss weights
    mse_weight = 1.0
    l1_weight = 0.1
    clip_weight = 0.5  # CLIP guidance weight
    
    print(f"ðŸŽ¯ CLIP-Guided Training Configuration:")
    print(f"   Epochs: {num_epochs}")
    print(f"   Batch size: 16")
    print(f"   Learning rate: 1e-4")
    print(f"   Loss weights: MSE={mse_weight}, L1={l1_weight}, CLIP={clip_weight}")
    print(f"   Total parameters: {sum(p.numel() for p in model.parameters()):,}")
    
    # Training loop
    start_time = time.time()
    
    for epoch in range(num_epochs):
        # Training phase
        model.train()
        epoch_train_loss = 0
        
        train_pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Train]")
        
        for eeg_emb, images, labels in train_pbar:
            eeg_emb = eeg_emb.to(device)
            images = images.to(device)
            labels = labels.to(device)
            
            # Sample random timesteps
            batch_size = images.shape[0]
            t = torch.randint(0, model.num_timesteps, (batch_size,), device=device).long()
            
            # Forward diffusion
            noisy_images, noise = model.forward_diffusion(images, t)
            
            # Predict noise
            noise_pred = model(noisy_images, t, eeg_emb)
            
            # Calculate reconstruction losses
            mse_loss = F.mse_loss(noise_pred, noise)
            l1_loss = F.l1_loss(noise_pred, noise)
            
            # Generate clean images for CLIP loss
            with torch.no_grad():
                # Denoise to get clean images
                alpha_t = model.alphas_cumprod[t][:, None, None, None]
                predicted_x0 = (noisy_images - torch.sqrt(1 - alpha_t) * noise_pred) / torch.sqrt(alpha_t)
                predicted_x0 = torch.clamp(predicted_x0, -1, 1)
            
            # Calculate CLIP loss
            clip_loss = clip_loss_fn(predicted_x0, labels)
            
            # Combined loss
            total_loss = (mse_weight * mse_loss + 
                         l1_weight * l1_loss + 
                         clip_weight * clip_loss)
            
            # Backward pass
            optimizer.zero_grad()
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            epoch_train_loss += total_loss.item()
            train_pbar.set_postfix({
                'total': f'{total_loss.item():.4f}',
                'mse': f'{mse_loss.item():.4f}',
                'clip': f'{clip_loss.item():.4f}',
                'lr': f'{optimizer.param_groups[0]["lr"]:.2e}'
            })
        
        avg_train_loss = epoch_train_loss / len(train_loader)
        train_losses.append(avg_train_loss)
        
        # Validation phase
        model.eval()
        epoch_test_loss = 0
        
        with torch.no_grad():
            test_pbar = tqdm(test_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Test]")
            
            for eeg_emb, images, labels in test_pbar:
                eeg_emb = eeg_emb.to(device)
                images = images.to(device)
                labels = labels.to(device)
                
                batch_size = images.shape[0]
                t = torch.randint(0, model.num_timesteps, (batch_size,), device=device).long()
                
                noisy_images, noise = model.forward_diffusion(images, t)
                noise_pred = model(noisy_images, t, eeg_emb)
                
                mse_loss = F.mse_loss(noise_pred, noise)
                l1_loss = F.l1_loss(noise_pred, noise)
                
                # Generate clean images for CLIP loss
                alpha_t = model.alphas_cumprod[t][:, None, None, None]
                predicted_x0 = (noisy_images - torch.sqrt(1 - alpha_t) * noise_pred) / torch.sqrt(alpha_t)
                predicted_x0 = torch.clamp(predicted_x0, -1, 1)
                
                clip_loss = clip_loss_fn(predicted_x0, labels)
                
                total_loss = (mse_weight * mse_loss + 
                             l1_weight * l1_loss + 
                             clip_weight * clip_loss)
                
                epoch_test_loss += total_loss.item()
                test_pbar.set_postfix({
                    'total': f'{total_loss.item():.4f}',
                    'mse': f'{mse_loss.item():.4f}',
                    'clip': f'{clip_loss.item():.4f}'
                })
        
        avg_test_loss = epoch_test_loss / len(test_loader)
        test_losses.append(avg_test_loss)
        
        # Learning rate scheduling
        scheduler.step()
        
        # Save best model
        if avg_test_loss < best_test_loss:
            best_test_loss = avg_test_loss
            torch.save(model.state_dict(), 'eeg_ldm_clip_guided_best.pth')
        
        # Print progress
        print(f"Epoch {epoch+1:3d}: Train Loss = {avg_train_loss:.4f}, Test Loss = {avg_test_loss:.4f}, LR = {optimizer.param_groups[0]['lr']:.2e}")
        
        # Save checkpoint every 10 epochs
        if (epoch + 1) % 10 == 0:
            torch.save(model.state_dict(), f'eeg_ldm_clip_guided_epoch_{epoch+1}.pth')
    
    training_time = time.time() - start_time
    
    # Save final model
    torch.save(model.state_dict(), 'eeg_ldm_clip_guided_final.pth')
    
    print(f"\nâœ… CLIP-guided training completed!")
    print(f"   Training time: {training_time:.2f} seconds")
    print(f"   Best test loss: {best_test_loss:.4f}")
    
    return model, train_losses, test_losses

def evaluate_clip_guided_ldm():
    """Evaluate CLIP-guided EEG LDM with semantic accuracy metrics"""
    print(f"\nðŸ“Š EVALUATING CLIP-GUIDED EEG LDM")
    print("=" * 50)

    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    # Load model
    model = CLIPGuidedEEGDiffusion(
        condition_dim=512,
        image_size=28,
        num_timesteps=100
    ).to(device)
    model.load_state_dict(torch.load('eeg_ldm_clip_guided_best.pth', map_location=device))
    model.eval()

    # Load test data with Crell embeddings
    test_dataset = CLIPGuidedEEGDataset(
        embeddings_file="crell_embeddings_20250622_173213.pkl",
        split="test",
        target_size=28
    )
    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)

    # Initialize CLIP for evaluation
    clip_loss_fn = CLIPGuidedLoss(device=device)

    # Generate predictions
    predictions = []
    targets = []
    labels = []

    print("ðŸŽ¨ Generating images with CLIP guidance...")

    with torch.no_grad():
        for i, (eeg_emb, images, label) in enumerate(tqdm(test_loader, desc="Generating")):
            if i >= 20:  # Generate first 20 for evaluation
                break

            eeg_emb = eeg_emb.to(device)

            # Generate image
            generated_images = model.sample(eeg_emb, num_inference_steps=20)

            predictions.append(generated_images.cpu().numpy())
            targets.append(images.numpy())
            labels.append(label.item())

    predictions = np.vstack(predictions)
    targets = np.vstack(targets)
    labels = np.array(labels)

    print(f"ðŸ“Š Generated predictions: {predictions.shape}")
    print(f"ðŸ“Š Target images: {targets.shape}")
    print(f"ðŸ“Š Labels: {labels}")

    # Calculate metrics
    mse = mean_squared_error(targets.flatten(), predictions.flatten())

    # Calculate correlation for each sample
    correlations = []
    for i in range(len(predictions)):
        pred_flat = predictions[i].flatten()
        target_flat = targets[i].flatten()
        corr, _ = pearsonr(pred_flat, target_flat)
        if not np.isnan(corr):
            correlations.append(corr)

    correlations = np.array(correlations)

    # Calculate semantic accuracy (if CLIP available)
    semantic_accuracy = 0.0
    if CLIP_AVAILABLE:
        with torch.no_grad():
            pred_tensor = torch.FloatTensor(predictions).to(device)

            # Calculate CLIP similarities for each digit
            correct_predictions = 0
            for i in range(len(predictions)):
                # Get CLIP features for generated image
                resized_img = F.interpolate(
                    pred_tensor[i:i+1],
                    size=(224, 224),
                    mode='bilinear',
                    align_corners=False
                )
                rgb_img = resized_img.repeat(1, 3, 1, 1)
                rgb_img = (rgb_img + 1) / 2  # Convert to [0, 1]

                img_features = clip_loss_fn.clip_model.encode_image(rgb_img)
                img_features = F.normalize(img_features, dim=-1)

                # Calculate similarities with all letter texts
                similarities = torch.matmul(img_features, clip_loss_fn.text_features.T)
                predicted_letter_idx = similarities.argmax().item()

                if predicted_letter_idx == labels[i]:
                    correct_predictions += 1

            semantic_accuracy = correct_predictions / len(predictions)

    print(f"ðŸ“Š CLIP-Guided EEG LDM Performance:")
    print(f"   MSE: {mse:.4f}")
    print(f"   Mean Correlation: {correlations.mean():.4f} Â± {correlations.std():.4f}")
    print(f"   Best Correlation: {correlations.max():.4f}")
    print(f"   Worst Correlation: {correlations.min():.4f}")
    if CLIP_AVAILABLE:
        print(f"   Semantic Accuracy: {semantic_accuracy:.4f} ({correct_predictions}/{len(predictions)})")

    # Letter mapping for display
    letter_mapping = {0: 'a', 1: 'd', 2: 'e', 3: 'f', 4: 'j', 5: 'n', 6: 'o', 7: 's', 8: 't', 9: 'v'}

    # Visualize results with labels
    fig, axes = plt.subplots(4, 4, figsize=(16, 16))
    fig.suptitle('CLIP-Guided EEG LDM Results: Original vs Generated Letters', fontsize=16)

    for i in range(4):
        if i < len(predictions):
            # Original
            orig_img = targets[i, 0]
            axes[0, i].imshow(orig_img, cmap='gray', vmin=-1, vmax=1)
            axes[0, i].set_title(f'Original {i+1}\nLetter: {letter_mapping[labels[i]]} ({labels[i]})')
            axes[0, i].axis('off')

            # Generated
            gen_img = predictions[i, 0]
            axes[1, i].imshow(gen_img, cmap='gray', vmin=-1, vmax=1)
            axes[1, i].set_title(f'Generated {i+1}\nCorr: {correlations[i]:.3f}')
            axes[1, i].axis('off')

            # Difference
            diff_img = np.abs(orig_img - gen_img)
            axes[2, i].imshow(diff_img, cmap='hot', vmin=0, vmax=1)
            axes[2, i].set_title(f'|Difference| {i+1}')
            axes[2, i].axis('off')

            # Label info
            axes[3, i].text(0.5, 0.5, f'Target: {letter_mapping[labels[i]]} ({labels[i]})\nCorr: {correlations[i]:.3f}',
                           ha='center', va='center', fontsize=12,
                           transform=axes[3, i].transAxes)
            axes[3, i].axis('off')

    plt.tight_layout()
    plt.savefig('eeg_ldm_clip_guided_results.png', dpi=300, bbox_inches='tight')
    plt.show()

    return predictions, targets, correlations, labels, semantic_accuracy

def main():
    """Main function"""
    print("ðŸŽ¯ CLIP-GUIDED EEG LATENT DIFFUSION MODEL")
    print("=" * 70)
    print("Using Crell EEG embeddings with CLIP guidance for letter generation")
    print("Letters: a, d, e, f, j, n, o, s, t, v (10 classes)")
    print("=" * 70)

    # Train model
    _, _, _ = train_clip_guided_ldm()

    # Evaluate model
    predictions, targets, correlations, _, semantic_accuracy = evaluate_clip_guided_ldm()

    print(f"\nðŸŽ¯ CLIP-GUIDED EEG LDM SUMMARY:")
    print(f"   Final MSE: {mean_squared_error(targets.flatten(), predictions.flatten()):.4f}")
    print(f"   Mean Correlation: {correlations.mean():.4f}")
    if CLIP_AVAILABLE:
        print(f"   Semantic Accuracy: {semantic_accuracy:.4f}")
    print(f"   Training completed successfully!")

    print(f"\nðŸ“ Generated files:")
    print(f"   - eeg_ldm_clip_guided_best.pth")
    print(f"   - eeg_ldm_clip_guided_final.pth")
    print(f"   - eeg_ldm_clip_guided_results.png")

    print(f"\nðŸš€ CLIP Guidance Benefits:")
    print(f"   âœ… Semantic accuracy guidance")
    print(f"   âœ… Correct digit generation")
    print(f"   âœ… Perceptual similarity")
    print(f"   âœ… Reduced semantic confusion")

    print(f"\nðŸ§  Expected Improvements:")
    print(f"   ðŸŽ¯ Correct digit mapping (1â†’1, 8â†’8, 4â†’4)")
    print(f"   ðŸ“ˆ Higher semantic accuracy")
    print(f"   ðŸŽ¨ Better perceptual quality")
    print(f"   ðŸ”„ Reduced label confusion")

if __name__ == "__main__":
    main()
