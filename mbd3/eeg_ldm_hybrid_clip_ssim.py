#!/usr/bin/env python3
"""
Hybrid CLIP-SSIM Guided EEG LDM
Combines CLIP semantic guidance with SSIM perceptual quality
Best of both worlds: semantic accuracy + visual quality
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
import pickle
import time
import math
from sklearn.metrics import mean_squared_error
from scipy.stats import pearsonr

class SSIMLoss(nn.Module):
    """SSIM Loss for better perceptual similarity"""
    
    def __init__(self, window_size=11, size_average=True):
        super(SSIMLoss, self).__init__()
        self.window_size = window_size
        self.size_average = size_average
        self.channel = 1
        self.window = self.create_window(window_size, self.channel)
        
    def gaussian(self, window_size, sigma):
        gauss = torch.Tensor([math.exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])
        return gauss/gauss.sum()
    
    def create_window(self, window_size, channel):
        _1D_window = self.gaussian(window_size, 1.5).unsqueeze(1)
        _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)
        window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()
        return window
    
    def _ssim(self, img1, img2, window, window_size, channel, size_average=True):
        mu1 = F.conv2d(img1, window, padding=window_size//2, groups=channel)
        mu2 = F.conv2d(img2, window, padding=window_size//2, groups=channel)
        
        mu1_sq = mu1.pow(2)
        mu2_sq = mu2.pow(2)
        mu1_mu2 = mu1*mu2
        
        sigma1_sq = F.conv2d(img1*img1, window, padding=window_size//2, groups=channel) - mu1_sq
        sigma2_sq = F.conv2d(img2*img2, window, padding=window_size//2, groups=channel) - mu2_sq
        sigma12 = F.conv2d(img1*img2, window, padding=window_size//2, groups=channel) - mu1_mu2
        
        C1 = 0.01**2
        C2 = 0.03**2
        
        ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))
        
        if size_average:
            return ssim_map.mean()
        else:
            return ssim_map.mean(1).mean(1).mean(1)
    
    def forward(self, img1, img2):
        (_, channel, _, _) = img1.size()
        
        if channel == self.channel and self.window.data.type() == img1.data.type():
            window = self.window
        else:
            window = self.create_window(self.window_size, channel)
            
            if img1.is_cuda:
                window = window.cuda(img1.get_device())
            window = window.type_as(img1)
            
            self.window = window
            self.channel = channel
        
        ssim_value = self._ssim(img1, img2, window, self.window_size, channel, self.size_average)
        return 1 - ssim_value  # Convert to loss (lower SSIM = higher loss)

# Import CLIP
try:
    import clip
    CLIP_AVAILABLE = True
    print("‚úÖ CLIP library available")
except ImportError:
    CLIP_AVAILABLE = False
    print("‚ö†Ô∏è CLIP not available, using classification approach")

# Import improved LDM components
from eeg_ldm_improved import ImprovedUNet

class HybridEEGDataset(Dataset):
    """Dataset for Hybrid CLIP-SSIM EEG LDM training"""
    
    def __init__(self, embeddings_file="eeg_embeddings_enhanced_20250622_123559.pkl", 
                 data_splits_file="explicit_eeg_data_splits.pkl", split="train", target_size=28):
        self.split = split
        self.target_size = target_size
        
        # Load EEG embeddings
        with open(embeddings_file, 'rb') as f:
            emb_data = pickle.load(f)
        
        # Load original data splits for stimulus images
        with open(data_splits_file, 'rb') as f:
            split_data = pickle.load(f)
        
        # Get split indices
        split_indices = emb_data['split_indices'][split]
        
        # Extract data for this split
        self.eeg_embeddings = emb_data['embeddings'][split_indices]
        self.labels = emb_data['labels'][split_indices]
        
        # Get stimulus images for this split
        if split == 'train':
            self.original_images = split_data['training']['stimTrn']
        elif split == 'val':
            self.original_images = split_data['validation']['stimVal']
        else:  # test
            self.original_images = split_data['test']['stimTest']
        
        print(f"üìä Loaded {split} data:")
        print(f"   EEG embeddings: {self.eeg_embeddings.shape}")
        print(f"   Labels: {self.labels.shape}")
        print(f"   Original images: {len(self.original_images)} PIL images")
        
        # Process images
        self.images = self._process_images()
        
        print(f"   Processed images: {self.images.shape}")
        print(f"   Image range: [{self.images.min():.3f}, {self.images.max():.3f}]")
    
    def _process_images(self):
        """Process PIL images to target format"""
        import numpy as np
        from PIL import Image
        
        processed_images = []
        
        for pil_img in self.original_images:
            # Convert PIL to numpy array
            img_array = np.array(pil_img)
            
            # Convert to grayscale if RGB
            if len(img_array.shape) == 3:
                img_array = np.mean(img_array, axis=2)  # RGB to grayscale
            
            # Resize to target size
            if img_array.shape[0] != self.target_size:
                pil_resized = Image.fromarray(img_array.astype(np.uint8)).resize(
                    (self.target_size, self.target_size), Image.LANCZOS)
                img_array = np.array(pil_resized)
            
            # Normalize to [-1, 1]
            img_array = img_array.astype(np.float32) / 255.0  # [0, 1]
            img_array = (img_array - 0.5) * 2  # [-1, 1]
            
            processed_images.append(img_array)
        
        # Stack and add channel dimension
        images = np.array(processed_images)  # (N, H, W)
        images = images[:, None, :, :]  # (N, 1, H, W)
        
        return images
    
    def __len__(self):
        return len(self.eeg_embeddings)
    
    def __getitem__(self, idx):
        eeg_emb = torch.FloatTensor(self.eeg_embeddings[idx])
        image = torch.FloatTensor(self.images[idx])
        label = self.labels[idx]
        
        return eeg_emb, image, label

class HybridCLIPSSIMLoss(nn.Module):
    """Hybrid loss combining CLIP semantic guidance with SSIM perceptual quality"""
    
    def __init__(self, device='cuda'):
        super().__init__()
        self.device = device
        
        # SSIM loss for perceptual similarity
        self.ssim_loss = SSIMLoss(window_size=11, size_average=True).to(device)
        
        # Enhanced classification head for semantic accuracy
        self.classifier = nn.Sequential(
            nn.Conv2d(1, 64, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Flatten(),
            nn.Linear(128 * 7 * 7, 256),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 10)
        ).to(device)
        
        if CLIP_AVAILABLE:
            # Load CLIP model
            self.clip_model, self.clip_preprocess = clip.load("ViT-B/32", device=device)
            self.clip_model.eval()
            
            # Enhanced digit text prompts with more variety
            self.digit_text_templates = [
                "a clear black digit {} on white background",
                "handwritten number {} in black ink",
                "the digit {} written clearly",
                "number {} in simple black font",
                "a bold digit {} on white paper",
                "handwritten {} in dark ink"
            ]
            
            # Encode all text prompts
            all_text_features = []
            with torch.no_grad():
                for digit in range(10):
                    digit_features = []
                    for template in self.digit_text_templates:
                        text = template.format(digit)
                        text_tokens = clip.tokenize([text]).to(device)
                        text_feat = self.clip_model.encode_text(text_tokens)
                        digit_features.append(text_feat)
                    
                    # Average multiple templates for each digit
                    avg_feat = torch.stack(digit_features).mean(dim=0)
                    all_text_features.append(avg_feat)
                
                self.text_features = torch.stack(all_text_features).squeeze(1)
                self.text_features = F.normalize(self.text_features, dim=-1)
            
            print(f"üìä CLIP text features: {self.text_features.shape}")
            print(f"üìä Using {len(self.digit_text_templates)} enhanced text templates per digit")
        else:
            self.clip_model = None
            print("üìä Using SSIM + classification loss only")
    
    def forward(self, generated_images, target_images, target_labels):
        """Calculate hybrid CLIP-SSIM loss"""
        # SSIM loss for perceptual similarity
        ssim_loss = self.ssim_loss(generated_images, target_images)
        
        # Classification loss for semantic accuracy
        logits = self.classifier(generated_images)
        classification_loss = F.cross_entropy(logits, target_labels)
        
        if CLIP_AVAILABLE and self.clip_model is not None:
            clip_loss = self._enhanced_clip_loss(generated_images, target_labels)
            # Hybrid combination: SSIM + Classification + CLIP
            total_loss = (0.4 * ssim_loss +           # Perceptual quality
                         0.4 * classification_loss +  # Direct semantic supervision
                         0.2 * clip_loss)             # CLIP semantic guidance
            return total_loss, ssim_loss, classification_loss, clip_loss
        else:
            total_loss = 0.6 * ssim_loss + 0.4 * classification_loss
            return total_loss, ssim_loss, classification_loss, torch.tensor(0.0, device=self.device)
    
    def _enhanced_clip_loss(self, generated_images, target_labels):
        """Enhanced CLIP-based semantic loss with better preprocessing"""
        # Resize images to 224x224 for CLIP
        resized_images = F.interpolate(
            generated_images, 
            size=(224, 224), 
            mode='bilinear', 
            align_corners=False
        )
        
        # Convert grayscale to RGB
        rgb_images = resized_images.repeat(1, 3, 1, 1)
        
        # Enhanced normalization for CLIP
        rgb_images = (rgb_images + 1) / 2  # Convert to [0, 1]
        # Apply CLIP normalization
        rgb_images = (rgb_images - 0.48145466) / 0.26862954  # CLIP mean/std normalization
        
        # Encode images
        with torch.no_grad():
            image_features = self.clip_model.encode_image(rgb_images)
            image_features = F.normalize(image_features, dim=-1)
        
        # Get target text features
        target_text_features = self.text_features[target_labels]
        
        # Calculate cosine similarity
        similarities = torch.sum(image_features * target_text_features, dim=-1)
        
        # Convert to loss (maximize similarity = minimize negative similarity)
        clip_loss = -similarities.mean()
        
        return clip_loss
    
    def predict_digit(self, generated_images):
        """Predict digit from generated images"""
        with torch.no_grad():
            logits = self.classifier(generated_images)
            predictions = torch.argmax(logits, dim=1)
            return predictions

class HybridCLIPSSIMEEGDiffusion(nn.Module):
    """Hybrid EEG diffusion model with CLIP semantic guidance + SSIM perceptual quality"""
    
    def __init__(self, condition_dim=512, image_size=28, num_timesteps=100):
        super().__init__()
        
        self.condition_dim = condition_dim
        self.image_size = image_size
        self.num_timesteps = num_timesteps
        
        # Enhanced UNet for noise prediction
        self.unet = ImprovedUNet(
            in_channels=1,
            out_channels=1,
            condition_dim=condition_dim
        )
        
        # Diffusion schedule
        self.register_buffer('betas', self._linear_beta_schedule(num_timesteps))
        self.register_buffer('alphas', 1.0 - self.betas)
        self.register_buffer('alphas_cumprod', torch.cumprod(self.alphas, dim=0))
        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(self.alphas_cumprod))
        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1.0 - self.alphas_cumprod))
        
        print(f"üìä Hybrid CLIP-SSIM EEG Diffusion Model:")
        print(f"   Total parameters: {sum(p.numel() for p in self.parameters()):,}")
        
    def _linear_beta_schedule(self, timesteps):
        """Linear beta schedule"""
        beta_start = 0.0001
        beta_end = 0.02
        return torch.linspace(beta_start, beta_end, timesteps)
    
    def forward_diffusion(self, x0, t, noise=None):
        """Add noise to images"""
        if noise is None:
            noise = torch.randn_like(x0)
        
        sqrt_alphas_cumprod_t = self.sqrt_alphas_cumprod[t][:, None, None, None]
        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t][:, None, None, None]
        
        return sqrt_alphas_cumprod_t * x0 + sqrt_one_minus_alphas_cumprod_t * noise, noise
    
    def forward(self, x, t, condition):
        """Predict noise"""
        return self.unet(x, t, condition)
    
    @torch.no_grad()
    def sample(self, condition, shape=None, num_inference_steps=20):
        """Generate image from EEG condition"""
        if shape is None:
            shape = (condition.shape[0], 1, self.image_size, self.image_size)
        
        device = condition.device
        
        # Start from noise
        x = torch.randn(shape, device=device)
        
        # DDIM sampling
        timesteps = torch.linspace(self.num_timesteps - 1, 0, num_inference_steps, dtype=torch.long, device=device)
        
        for i, t in enumerate(tqdm(timesteps, desc="Sampling", leave=False)):
            t_batch = t.repeat(shape[0])
            
            # Predict noise
            noise_pred = self.unet(x, t_batch, condition)
            
            # DDIM step
            if i < len(timesteps) - 1:
                alpha_t = self.alphas_cumprod[t]
                alpha_t_next = self.alphas_cumprod[timesteps[i + 1]]
                
                x = torch.sqrt(alpha_t_next) * (x - torch.sqrt(1 - alpha_t) * noise_pred) / torch.sqrt(alpha_t) + \
                    torch.sqrt(1 - alpha_t_next) * noise_pred
            else:
                # Final step
                alpha_t = self.alphas_cumprod[t]
                x = (x - torch.sqrt(1 - alpha_t) * noise_pred) / torch.sqrt(alpha_t)
        
        return torch.clamp(x, -1, 1)

def train_hybrid_clip_ssim_ldm():
    """Train Hybrid CLIP-SSIM EEG LDM for optimal semantic accuracy + perceptual quality"""
    print("üöÄ TRAINING HYBRID CLIP-SSIM EEG LDM")
    print("=" * 80)

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"üì± Device: {device}")

    # Load datasets
    train_dataset = HybridEEGDataset(split="train", target_size=28)
    test_dataset = HybridEEGDataset(split="test", target_size=28)

    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

    # Initialize models
    model = HybridCLIPSSIMEEGDiffusion(
        condition_dim=512,
        image_size=28,
        num_timesteps=100
    ).to(device)

    hybrid_loss_fn = HybridCLIPSSIMLoss(device=device)

    # Training setup
    optimizer = optim.AdamW(
        list(model.parameters()) + list(hybrid_loss_fn.parameters()),
        lr=1e-4,
        weight_decay=1e-4
    )
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)

    # Training parameters
    num_epochs = 100  # More epochs for better convergence
    best_test_loss = float('inf')
    train_losses = []
    test_losses = []

    # Optimized loss weights for hybrid approach
    l1_weight = 0.1  # Small L1 for fine details

    print(f"üéØ Hybrid CLIP-SSIM Training Configuration:")
    print(f"   Epochs: {num_epochs}")
    print(f"   Batch size: 16")
    print(f"   Learning rate: 1e-4")
    print(f"   Hybrid loss weights: SSIM=0.4, Classification=0.4, CLIP=0.2")
    print(f"   L1 weight: {l1_weight}")
    print(f"   Total parameters: {sum(p.numel() for p in model.parameters()):,}")

    # Training loop
    start_time = time.time()

    for epoch in range(num_epochs):
        # Training phase
        model.train()
        hybrid_loss_fn.train()
        epoch_train_loss = 0

        train_pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Train]")

        for eeg_emb, images, labels in train_pbar:
            eeg_emb = eeg_emb.to(device)
            images = images.to(device)
            labels = labels.to(device)

            # Sample random timesteps
            batch_size = images.shape[0]
            t = torch.randint(0, model.num_timesteps, (batch_size,), device=device).long()

            # Forward diffusion
            noisy_images, noise = model.forward_diffusion(images, t)

            # Predict noise
            noise_pred = model(noisy_images, t, eeg_emb)

            # Calculate L1 loss for noise prediction
            l1_loss = F.l1_loss(noise_pred, noise)

            # Generate clean images for hybrid loss
            with torch.no_grad():
                # Denoise to get clean images
                alpha_t = model.alphas_cumprod[t][:, None, None, None]
                predicted_x0 = (noisy_images - torch.sqrt(1 - alpha_t) * noise_pred) / torch.sqrt(alpha_t)
                predicted_x0 = torch.clamp(predicted_x0, -1, 1)

            # Calculate hybrid loss (SSIM + Classification + CLIP)
            hybrid_loss, ssim_loss, class_loss, clip_loss = hybrid_loss_fn(predicted_x0, images, labels)

            # Combined loss
            total_loss = hybrid_loss + l1_weight * l1_loss

            # Backward pass
            optimizer.zero_grad()
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(
                list(model.parameters()) + list(hybrid_loss_fn.parameters()), 1.0
            )
            optimizer.step()

            epoch_train_loss += total_loss.item()
            train_pbar.set_postfix({
                'total': f'{total_loss.item():.4f}',
                'hybrid': f'{hybrid_loss.item():.4f}',
                'ssim': f'{ssim_loss.item():.4f}',
                'class': f'{class_loss.item():.4f}',
                'clip': f'{clip_loss.item():.4f}',
                'lr': f'{optimizer.param_groups[0]["lr"]:.2e}'
            })

        avg_train_loss = epoch_train_loss / len(train_loader)
        train_losses.append(avg_train_loss)

        # Validation phase
        model.eval()
        hybrid_loss_fn.eval()
        epoch_test_loss = 0

        with torch.no_grad():
            test_pbar = tqdm(test_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Test]")

            for eeg_emb, images, labels in test_pbar:
                eeg_emb = eeg_emb.to(device)
                images = images.to(device)
                labels = labels.to(device)

                batch_size = images.shape[0]
                t = torch.randint(0, model.num_timesteps, (batch_size,), device=device).long()

                noisy_images, noise = model.forward_diffusion(images, t)
                noise_pred = model(noisy_images, t, eeg_emb)

                l1_loss = F.l1_loss(noise_pred, noise)

                # Generate clean images for hybrid loss
                alpha_t = model.alphas_cumprod[t][:, None, None, None]
                predicted_x0 = (noisy_images - torch.sqrt(1 - alpha_t) * noise_pred) / torch.sqrt(alpha_t)
                predicted_x0 = torch.clamp(predicted_x0, -1, 1)

                hybrid_loss, ssim_loss, class_loss, clip_loss = hybrid_loss_fn(predicted_x0, images, labels)

                total_loss = hybrid_loss + l1_weight * l1_loss

                epoch_test_loss += total_loss.item()
                test_pbar.set_postfix({
                    'total': f'{total_loss.item():.4f}',
                    'hybrid': f'{hybrid_loss.item():.4f}',
                    'ssim': f'{ssim_loss.item():.4f}',
                    'class': f'{class_loss.item():.4f}',
                    'clip': f'{clip_loss.item():.4f}'
                })

        avg_test_loss = epoch_test_loss / len(test_loader)
        test_losses.append(avg_test_loss)

        # Learning rate scheduling
        scheduler.step()

        # Save best model
        if avg_test_loss < best_test_loss:
            best_test_loss = avg_test_loss
            torch.save({
                'model': model.state_dict(),
                'hybrid_loss': hybrid_loss_fn.state_dict()
            }, 'eeg_ldm_hybrid_clip_ssim_best.pth')

        # Print progress
        print(f"Epoch {epoch+1:3d}: Train Loss = {avg_train_loss:.4f}, Test Loss = {avg_test_loss:.4f}, LR = {optimizer.param_groups[0]['lr']:.2e}")

        # Save checkpoint every 20 epochs
        if (epoch + 1) % 20 == 0:
            torch.save({
                'model': model.state_dict(),
                'hybrid_loss': hybrid_loss_fn.state_dict()
            }, f'eeg_ldm_hybrid_clip_ssim_epoch_{epoch+1}.pth')

    training_time = time.time() - start_time

    # Save final model
    torch.save({
        'model': model.state_dict(),
        'hybrid_loss': hybrid_loss_fn.state_dict()
    }, 'eeg_ldm_hybrid_clip_ssim_final.pth')

    print(f"\n‚úÖ Hybrid CLIP-SSIM training completed!")
    print(f"   Training time: {training_time:.2f} seconds")
    print(f"   Best test loss: {best_test_loss:.4f}")

    return model, hybrid_loss_fn, train_losses, test_losses

def calculate_ssim_metric(img1, img2):
    """Calculate SSIM metric between two images"""
    ssim_loss_fn = SSIMLoss(window_size=11, size_average=True)
    ssim_loss = ssim_loss_fn(img1, img2)
    ssim_value = 1 - ssim_loss.item()  # Convert loss back to SSIM
    return ssim_value

def evaluate_hybrid_clip_ssim_ldm():
    """Evaluate Hybrid CLIP-SSIM EEG LDM with comprehensive metrics"""
    print(f"\nüìä EVALUATING HYBRID CLIP-SSIM EEG LDM")
    print("=" * 70)

    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    # Load models
    model = HybridCLIPSSIMEEGDiffusion(
        condition_dim=512,
        image_size=28,
        num_timesteps=100
    ).to(device)

    hybrid_loss_fn = HybridCLIPSSIMLoss(device=device)

    # Load best checkpoint
    checkpoint = torch.load('eeg_ldm_hybrid_clip_ssim_best.pth', map_location=device)
    model.load_state_dict(checkpoint['model'])
    hybrid_loss_fn.load_state_dict(checkpoint['hybrid_loss'])

    model.eval()
    hybrid_loss_fn.eval()

    # Load test data
    test_dataset = HybridEEGDataset(split="test", target_size=28)
    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)

    # Generate predictions
    predictions = []
    targets = []
    labels = []

    print("üé® Generating images with hybrid CLIP-SSIM guidance...")

    with torch.no_grad():
        for i, (eeg_emb, images, label) in enumerate(tqdm(test_loader, desc="Generating")):
            if i >= 20:  # Generate first 20 for evaluation
                break

            eeg_emb = eeg_emb.to(device)

            # Generate image
            generated_images = model.sample(eeg_emb, num_inference_steps=20)

            predictions.append(generated_images.cpu().numpy())
            targets.append(images.numpy())
            labels.append(label.item())

    predictions = np.vstack(predictions)
    targets = np.vstack(targets)
    labels = np.array(labels)

    print(f"üìä Generated predictions: {predictions.shape}")
    print(f"üìä Target images: {targets.shape}")
    print(f"üìä Labels: {labels}")

    # Calculate comprehensive metrics
    mse = mean_squared_error(targets.flatten(), predictions.flatten())

    # Calculate correlation and SSIM for each sample
    correlations = []
    ssim_values = []
    for i in range(len(predictions)):
        pred_flat = predictions[i].flatten()
        target_flat = targets[i].flatten()
        corr, _ = pearsonr(pred_flat, target_flat)
        if not np.isnan(corr):
            correlations.append(corr)

        # Calculate SSIM for each sample
        pred_tensor = torch.FloatTensor(predictions[i:i+1]).to(device)
        target_tensor = torch.FloatTensor(targets[i:i+1]).to(device)
        ssim_val = calculate_ssim_metric(pred_tensor, target_tensor)
        ssim_values.append(ssim_val)

    correlations = np.array(correlations)
    ssim_values = np.array(ssim_values)

    # Calculate semantic accuracy using classification head
    semantic_accuracy = 0.0
    predicted_digits = []

    with torch.no_grad():
        pred_tensor = torch.FloatTensor(predictions).to(device)
        predicted_digit_logits = hybrid_loss_fn.predict_digit(pred_tensor)
        predicted_digits = predicted_digit_logits.cpu().numpy()

        correct_predictions = (predicted_digits == labels).sum()
        semantic_accuracy = correct_predictions / len(predictions)

    print(f"üìä Hybrid CLIP-SSIM EEG LDM Performance:")
    print(f"   MSE: {mse:.4f}")
    print(f"   Mean Correlation: {correlations.mean():.4f} ¬± {correlations.std():.4f}")
    print(f"   Best Correlation: {correlations.max():.4f}")
    print(f"   Worst Correlation: {correlations.min():.4f}")
    print(f"   Mean SSIM: {ssim_values.mean():.4f} ¬± {ssim_values.std():.4f}")
    print(f"   Best SSIM: {ssim_values.max():.4f}")
    print(f"   Worst SSIM: {ssim_values.min():.4f}")
    print(f"   Semantic Accuracy: {semantic_accuracy:.4f} ({correct_predictions}/{len(predictions)})")

    # Visualize results with comprehensive metrics
    fig, axes = plt.subplots(6, 4, figsize=(16, 24))
    fig.suptitle('Hybrid CLIP-SSIM EEG LDM: Best of Both Worlds', fontsize=16, fontweight='bold')

    for i in range(4):
        if i < len(predictions):
            # Original
            orig_img = targets[i, 0]
            axes[0, i].imshow(orig_img, cmap='gray', vmin=-1, vmax=1)
            axes[0, i].set_title(f'Original {i+1}\nTrue Label: {labels[i]}', fontweight='bold')
            axes[0, i].axis('off')

            # Generated
            gen_img = predictions[i, 0]
            axes[1, i].imshow(gen_img, cmap='gray', vmin=-1, vmax=1)
            correct_symbol = "‚úÖ" if predicted_digits[i] == labels[i] else "‚ùå"
            axes[1, i].set_title(f'Generated {i+1}\nPredicted: {predicted_digits[i]} {correct_symbol}', fontweight='bold')
            axes[1, i].axis('off')

            # Difference
            diff_img = np.abs(orig_img - gen_img)
            axes[2, i].imshow(diff_img, cmap='hot', vmin=0, vmax=1)
            axes[2, i].set_title(f'|Difference| {i+1}')
            axes[2, i].axis('off')

            # Semantic metrics
            axes[3, i].text(0.5, 0.8, f'True: {labels[i]}', ha='center', va='center',
                           fontsize=14, fontweight='bold', transform=axes[3, i].transAxes)
            axes[3, i].text(0.5, 0.6, f'Pred: {predicted_digits[i]} {correct_symbol}',
                           ha='center', va='center', fontsize=14, fontweight='bold',
                           transform=axes[3, i].transAxes)
            axes[3, i].text(0.5, 0.4, f'Accuracy: {semantic_accuracy:.1%}',
                           ha='center', va='center', fontsize=12,
                           transform=axes[3, i].transAxes)
            axes[3, i].axis('off')

            # Quality metrics
            axes[4, i].text(0.5, 0.8, f'Correlation: {correlations[i]:.3f}',
                           ha='center', va='center', fontsize=12,
                           transform=axes[4, i].transAxes)
            axes[4, i].text(0.5, 0.6, f'SSIM: {ssim_values[i]:.3f}',
                           ha='center', va='center', fontsize=12,
                           transform=axes[4, i].transAxes)
            axes[4, i].text(0.5, 0.4, f'MSE: {mean_squared_error(orig_img.flatten(), gen_img.flatten()):.3f}',
                           ha='center', va='center', fontsize=12,
                           transform=axes[4, i].transAxes)
            axes[4, i].axis('off')

            # Overall summary
            axes[5, i].text(0.5, 0.7, f'Hybrid Model',
                           ha='center', va='center', fontsize=14, fontweight='bold',
                           transform=axes[5, i].transAxes)
            axes[5, i].text(0.5, 0.5, f'CLIP + SSIM',
                           ha='center', va='center', fontsize=12,
                           transform=axes[5, i].transAxes)
            axes[5, i].text(0.5, 0.3, f'Semantic + Quality',
                           ha='center', va='center', fontsize=12,
                           transform=axes[5, i].transAxes)
            axes[5, i].axis('off')

    plt.tight_layout()
    plt.savefig('eeg_ldm_hybrid_clip_ssim_results.png', dpi=300, bbox_inches='tight')
    plt.show()

    return predictions, targets, correlations, labels, semantic_accuracy, predicted_digits, ssim_values

def main():
    """Main function"""
    print("üéØ HYBRID CLIP-SSIM EEG LATENT DIFFUSION MODEL")
    print("=" * 90)
    print("üöÄ Best of Both Worlds: CLIP Semantic Guidance + SSIM Perceptual Quality")
    print("=" * 90)

    # Train model
    _, _, _, _ = train_hybrid_clip_ssim_ldm()

    # Evaluate model
    predictions, targets, correlations, _, semantic_accuracy, _, ssim_values = evaluate_hybrid_clip_ssim_ldm()

    print(f"\nüéØ HYBRID CLIP-SSIM EEG LDM SUMMARY:")
    print(f"   Final MSE: {mean_squared_error(targets.flatten(), predictions.flatten()):.4f}")
    print(f"   Mean Correlation: {correlations.mean():.4f}")
    print(f"   Mean SSIM: {ssim_values.mean():.4f}")
    print(f"   Semantic Accuracy: {semantic_accuracy:.4f}")
    print(f"   Training completed successfully!")

    print(f"\nüìÅ Generated files:")
    print(f"   - eeg_ldm_hybrid_clip_ssim_best.pth")
    print(f"   - eeg_ldm_hybrid_clip_ssim_final.pth")
    print(f"   - eeg_ldm_hybrid_clip_ssim_results.png")

    print(f"\nüöÄ Hybrid CLIP-SSIM Benefits:")
    print(f"   ‚úÖ CLIP semantic guidance for correct digit generation")
    print(f"   ‚úÖ SSIM perceptual quality for better visual appearance")
    print(f"   ‚úÖ Enhanced classification head for direct supervision")
    print(f"   ‚úÖ Balanced loss weighting (SSIM=0.4, Class=0.4, CLIP=0.2)")
    print(f"   ‚úÖ Enhanced CLIP preprocessing with proper normalization")
    print(f"   ‚úÖ 6 diverse text templates per digit")

    print(f"\nüß† Expected Hybrid Advantages:")
    print(f"   üéØ Higher semantic accuracy (target >30%)")
    print(f"   üìà Better visual quality (higher SSIM)")
    print(f"   üé® More natural-looking digits")
    print(f"   üîÑ Optimal balance of accuracy and quality")
    print(f"   üí™ Best generative model performance")

if __name__ == "__main__":
    main()
